LSTM-Based Text Classification

ğŸ“Œ Project Overview

This project develops a Long Short-Term Memory (LSTM) neural network model for text classification. The model categorizes text into four predefined categories: Electronics, Household, Books, and Clothing & Accessories. The goal is to achieve:

âœ… Accuracy: > 85%
âœ… F1 Score: > 0.7

![alt text](image/classification_reportF1.png)

ğŸ“‚ Project Structure

â”œâ”€â”€ saved_models/          # Directory to store trained models
â”‚   â”œâ”€â”€ lstm_model.h5      # Trained LSTM model in HDF5 format
â”‚   â”œâ”€â”€ lstm_model.keras   # Trained LSTM model in Keras format
â”‚   â”œâ”€â”€ tokenizer.json     # Tokenizer configuration
â”‚   â”œâ”€â”€ label_encoder.json # Label encoder mappings
â”œâ”€â”€ data/                  # Dataset folder
â”‚   â”œâ”€â”€ dataset.csv        # Preprocessed dataset
â”œâ”€â”€ logs/                  # TensorBoard logs for visualization
â”œâ”€â”€ main.py                # Main script for training and evaluation
â”œâ”€â”€ requirements.txt       # Required dependencies
â””â”€â”€ README.md              # Project documentation

ğŸ›  Setup Instructions

1ï¸âƒ£ Install Dependencies

Ensure you have Python 3.7+ installed, then run:

pip install -r requirements.txt

2ï¸âƒ£ Train the Model

Run the main script to train and evaluate the model:

python main.py

3ï¸âƒ£ View Training Progress on TensorBoard

To visualize training logs:

tensorboard --logdir=logs

ğŸ“Š Model Architecture

The model consists of the following layers:

TextVectorization: Converts text into numerical sequences

Embedding Layer: Transforms sequences into dense vector representations

Bidirectional LSTM: Captures long-term dependencies in text

Dropout Layers: Prevents overfitting

Dense Layers: Processes extracted features

ğŸ” Performance & Evaluation

The model is evaluated based on:

Accuracy: Measures correct predictions

F1 Score: Balances precision and recall

Confusion Matrix: Visualizes misclassifications

ğŸ† Optimizations Implemented

âœ” Dropout Regularization to reduce overfitting
âœ” Early Stopping with patience = 3
âœ” Reduce Learning Rate when validation loss plateaus
âœ” Bidirectional LSTM for better context understanding



ğŸš€ Future Improvements

Implement pretrained embeddings (GloVe, FastText)

Explore GRU instead of LSTM

Apply data augmentation for better generalization

ğŸ“ License
This project is open-source 
