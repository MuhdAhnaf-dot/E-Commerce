LSTM-Based Text Classification

ğŸ“Œ Project Overview
E-commerce (electronic commerce) refers to the buying and selling of goods and services over the internet. It has revolutionized the way businesses operate, allowing transactions to take place across borders with ease.

ğŸ“¦ Role of Product Categorization in E-Commerce
In an e-commerce platform, product categorization is a crucial task to enhance user experience, optimize search functionality, and improve recommendation systems. Manually categorizing thousands of products can be time-consuming and error-prone, which is why machine learning models like LSTMs (Long Short-Term Memory networks) are used to automate and improve classification accuracy.

This project focuses on classifying e-commerce products into four main categories:
âœ… Electronics
âœ… Household
âœ… Books
âœ… Clothing & Accessories

By leveraging Natural Language Processing (NLP) and LSTM models, the goal is to automate this classification process with high accuracy.

âœ… Accuracy: > 85%
âœ… F1 Score: > 0.7

![alt text](image/classification_reportF1.png)

ğŸ“‚ Project Structure

â”œâ”€â”€ saved_models/          # Directory to store trained models
â”‚   â”œâ”€â”€ lstm_model.h5      # Trained LSTM model in HDF5 format
â”‚   â”œâ”€â”€ lstm_model.keras   # Trained LSTM model in Keras format
â”‚   â”œâ”€â”€ tokenizer.json     # Tokenizer configuration
â”‚   â”œâ”€â”€ label_encoder.json # Label encoder mappings
â”œâ”€â”€ data/                  # Dataset folder
â”‚   â”œâ”€â”€ dataset.csv        # Preprocessed dataset
â”œâ”€â”€ logs/                  # TensorBoard logs for visualization
â”œâ”€â”€ main.py                # Main script for training and evaluation
â”œâ”€â”€ requirements.txt       # Required dependencies
â””â”€â”€ README.md              # Project documentation

ğŸ›  Setup Instructions

1ï¸âƒ£ Install Dependencies

Ensure you have Python 3.7+ installed, then run:

pip install -r requirements.txt

2ï¸âƒ£ Train the Model

Run the main script to train and evaluate the model:

python main.py

3ï¸âƒ£ View Training Progress on TensorBoard

To visualize training logs:

tensorboard --logdir=logs

ğŸ“Š Model Architecture

The model consists of the following layers:

TextVectorization: Converts text into numerical sequences

Embedding Layer: Transforms sequences into dense vector representations

Bidirectional LSTM: Captures long-term dependencies in text

Dropout Layers: Prevents overfitting

Dense Layers: Processes extracted features

ğŸ” Performance & Evaluation

The model is evaluated based on:

Accuracy: Measures correct predictions

F1 Score: Balances precision and recall

Confusion Matrix: Visualizes misclassifications

ğŸ† Optimizations Implemented

âœ” Dropout Regularization to reduce overfitting
âœ” Early Stopping with patience = 3
âœ” Reduce Learning Rate when validation loss plateaus
âœ” Bidirectional LSTM for better context understanding

Result for loss and accuracy:

![alt text](image/epoch_accuracy.png)
![alt text](image/epoch_loss.png)


ğŸš€ Future Improvements

Implement pretrained embeddings (GloVe, FastText)

Explore GRU instead of LSTM

Apply data augmentation for better generalization

ğŸ“ License
This project is open-source 
